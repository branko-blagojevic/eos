Clones for Backups
------------------

Short summary:

file modification times are tracked independent of mtime, a hierarchy of files
modified after a certain date can be isolated in a snap-shot with copy-on-write
semantics, an easily parsable report can feed a back-up script. 



1. syncTime

a "server-store-timestamp" is maintained for all directories and files. This is
conceptually different from mtime (which can be set by the user), however: for
containers the TMTime field is used; for files 'syncTime' has been added - but
it is only different from the default (hence mostly not stored) if mtime is set
explicitly, which should be rare. Thus, in most practical cases syncTime will be
mtime, minimising the increase in quarkdb footprint. While a clone exists,
transient "cloneId" and possibly "cloneFST" attributes are maintained in the
mgm.

2. the clone

the clone is a "sparse" or "lazy" clone, starting empty. Upon creation, all
files are tagged with the cloneId field. When files are modified, their original
contents are "reflinked" on the FSTs (see "cp --reflink", which implements a
copy-on-write on the original FST) into a new file, which is placed in a
container modelled after their parent containers
/eos/<instance>/proc/clone/cloneId. For each file the clone process returns the
foreseen clone path, for use in backup scripts. 

3. cloning in hierarchies

Cloning starts at directory level. All files at directory level are tagged with
the cloneId. The cloning process then recursively descends into subdirectories.
To prevent this, a directory can be flagged with a different cloneId, or a
special "dummy" cloneId used as a marker on a directory preventing descent.  

4. clone process details 

A cloneId is a [unique] second-resolution timestamp generated automatically at
the beginning of the cloning process. The clone process is triggered by a find
command with the '-x sys.clone=+...' option (specifying sys.clone requires
privileges):

  "-x sys.clone=? ..."
        - displays all files and clone attributes under ...,
  "-x sys.clone=-1556195651 ..."
        - deletes all traces of the clone 1556195651,
  "-x sys.clone=+10 ..."
        - makes a full backup clone (numbers < 10 are markers,
            from 10 onwards second-precision timestamps),
  "-x sys.clone=+1 ..."
        - sets a marker not to descend into ...  while cloning,
  "-x sys.clone=+1556195651 ..."
        - makes a clone of all files altered after 1556195651.0

Cloning tags every file and directory in the hierarchy with the (generated)
cloneId.

5. copy-on-write

Whenever a file is being deleted, or being rewritten using OTRUNCATE, its
underlying data are transferred (using a low-cost "mv") to a clone file under a
predefined name in the clone hierarchy. When rewritten with OTRUNCATE, a new
file will then automatically be created, tagged as "cloned". When a file is
updated (modified) without OTRUNCATE, a copy-on-write clone is created, using
reflink (which is low-cost in recent file systems, but elsewhere a real copy). 

6. the backup process

"find -f -x sys.clone=+nnnnnnnn /path" clones a directory tree and creates an
easily parsable output for reference, indicating which files should be backed
up. As soon as the clone has been created, write operations to files may result
in copy-on-write clones be created, hence during back-up a decision has to be
made whether to back up a possible clone file or, for a full backup, the live
file. For a live file, the decision which may have to be revised after the copy
if the clone has been created in the meantime.

7. cleanup

After the backup, 'find -x sys.clone=-nnnnnnnn' deletes all the cloneId and
clone-tag attributes and cleans the clone directory under /proc/clone/cloneId.
The temporary clones disappear from the FSTs by-and-by. The back-up now only
resides on backup media. 


Here's a complete backup and restore recipe, including incremental (as
implemented by test/eos-backup-test):

a. backup triggers a full backup issuing 'find -f -x sys.clone=+10 <path>' and
stores the output retrieved from the top level directory for reference. The
output are structured lines of (filename, ' ', mtime, ':', cloneId/clonePath,
':', somethingElse). The backupId is the cloneId returned for the top-level
directory (the first line). For each file, backup copies the version under the
clone path to backup media if it exist, or the live file otherwise. If the live
file has been copied, a check is performed again after the copy to assess no
clone file has been created meanwhile, or the clone file replaces the live file
on backup media. The filename may be escaped if needed for the output to remain
parsable (scan for '%'). <path> is backed up recursively, descent into
subdirectories can be stopped with a directory marker (see above). A filename
ending in a '/' is a directory. The clonePath is relative to
/eos/<instance>/proc/clone and structured backupId/cloneDir/cloneFile for files,
or simply backupId for directories in the report;

b. backup then cleans the backup clone: find -f -x sys.clone=-nnnnnnnn <path>.
The directory returns to its normal EOS state (without copy-on-write overhead);

c. incremental backups may be triggered using 'find -f -x
sys.clone=+<previousBackupId> <path>'. The previousBackupId would either be the
backupId of a previous backup or the backupId of the full backup, depending on
the type of increment desired. Again, the backupId of the incremental backup is
the cloneId of the top-level (first) directory. An incremental backup reports
*all* current files below <path>. Those not modified since the previous backup
can be distinguished in the output by a backupId (often '0') that differs from
the top-level directory - no need to back them up again;

d. restore collects all stored backup reports (full, incremental 1,
incremental2, ... up-to-desired-restore date) for <path> into one list sorted by
filename and backupId. The last (!) incremental report contains the *final* list
of files to be restored.  For every file in the sorted list, only the most
recent (highest backupId) is restored from backup media (and only if it appears
in the final list. At this point a filter could be applied to the list to only
restore selected files.




In short:

- metadata operations under (ideally) the biggest lock possible for consistency
- the standard write may be delayed for copy-on-write - slow with ancient kernels
- copy-on-write clones are short lived, and only needed for partially modified files
- the backup will be "largely" consistent, even without a BIG lock

Timing in current implementation (in seconds), 100 files in 100 directories
(=10000 files), 30 bytes each (`date`), dockertest-instance, bash script, using
fusex:

creation:           105         /eos/rtb/tobbicke/backuptest/t??/tt??
append `date`:      115
clone creation:     1
append `date`:      443
append `date`:      147
"wc -c" on clones:  45          /eos/dockertest/proc/clone/1558357820/Dxxx/Fxxx         
"wc -c" on live files:  49
remove all clones:  2

The cloning time alone is not visible here, since it includes formatting and
displaying the result. Tests would be needed on bigger instances to evaluate the
impact of mgm locking.

The "append date" processes naturally increase in duration - there are more data
to ship to the FSTs. The first run after the clone actually includes creating
the clones - hence it is more "expensive".
